{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YMbXyPk6Amvo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "XzMB4QwG1aWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dtania/ua-gec-workshop.git"
      ],
      "metadata": {
        "id": "ZuwzqalqU-yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ua-gec-workshop/requirements.txt"
      ],
      "metadata": {
        "id": "_UZp_CheU-1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data processing functions**"
      ],
      "metadata": {
        "id": "_QePUOw62oGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import time\n",
        "import numpy as np\n",
        "import subprocess\n",
        "\n",
        "from collections import Counter\n",
        "from google import genai\n",
        "from google.genai import errors\n",
        "from langchain_core.prompts.prompt import PromptTemplate\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "3TYeml4j2unm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(\"workshop_logger\")\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.propagate = False\n",
        "\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "file_handler = logging.FileHandler(\"workshop_run.log\", mode=\"w\", encoding=\"utf-8\")\n",
        "file_handler.setFormatter(formatter)\n",
        "\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setFormatter(formatter)\n",
        "\n",
        "logger.addHandler(file_handler)\n",
        "logger.addHandler(stream_handler)"
      ],
      "metadata": {
        "id": "qDQ8a1GOl2bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(input_file):\n",
        "  \"\"\"\n",
        "  Reads a text file and returns a list of stripped sentences.\n",
        "  \"\"\"\n",
        "  with open(input_file, \"r\") as fp:\n",
        "    return [sentence.strip() for sentence in fp.readlines()]"
      ],
      "metadata": {
        "id": "Kr-KWI4fuMKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(prompt, max_retries=7):\n",
        "    \"\"\"\n",
        "    Runs the prompt and returns the response from the API, handling rate limits with retries.\n",
        "    \"\"\"\n",
        "    retries = 0\n",
        "    client = genai.Client(api_key=\"<add_your_API_key_here>\")\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = client.models.generate_content(\n",
        "                model=\"gemma-3-27b-it\",\n",
        "                contents=prompt,\n",
        "            )\n",
        "            return response.text.strip()\n",
        "\n",
        "        except errors.APIError as e:\n",
        "            wait_time = 3 ** retries\n",
        "            logging.warning(f\"Error: {e.message}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "            retries += 1\n",
        "\n",
        "    raise Exception(\"Max retries reached. Failed to get a response.\")"
      ],
      "metadata": {
        "id": "wiGl6GkOATp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_batch(output_file, data, prompt):\n",
        "  \"\"\"\n",
        "  Processes a batch of text inputs and writes the results both to a new file and a list.\n",
        "  \"\"\"\n",
        "  corrected_sentences = []\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
        "      for text in tqdm(data, desc=\"Processing\"):\n",
        "          response = generate_response(prompt.format(data=text))\n",
        "          corrected_sentences.append(response)\n",
        "          file.write(response + \"\\n\")\n",
        "\n",
        "  return corrected_sentences"
      ],
      "metadata": {
        "id": "b_owpZzmrHW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_errant_metric(pred_file, m2_file):\n",
        "    \"\"\"\n",
        "    Runs the errant evaluation script and returns stdout as a string.\n",
        "    \"\"\"\n",
        "    command = [\n",
        "        \"python\",\n",
        "        \"ua-gec-workshop/utils/evaluate.py\",\n",
        "        pred_file,\n",
        "        \"--m2\",\n",
        "        m2_file,\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "        return result.stdout\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        logger.error(\"Evaluation script failed with error:\\n%s\", e.stderr)\n",
        "        raise"
      ],
      "metadata": {
        "id": "iQcwsXFzQSX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the model for computing cosine similarity only once\n",
        "transformers_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")"
      ],
      "metadata": {
        "id": "Ux9Dj499tFN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(references, hypotheses, model: SentenceTransformer):\n",
        "    \"\"\"\n",
        "    Computes cosine similarity for corresponding pairs of reference and hypothesis sentences\n",
        "    using a provided sentence embedding model. Returns the average, minimum, and maximum similarity,\n",
        "    and also includes the indices of the reference and hypothesis that produced the min and max scores.\n",
        "    \"\"\"\n",
        "    if len(references) != len(hypotheses):\n",
        "        raise ValueError(\"The number of references and hypotheses must be the same.\")\n",
        "\n",
        "    if not references:\n",
        "        raise ValueError(\"The input lists must not be empty.\")\n",
        "\n",
        "    # Compute embeddings for the lists of sentences for efficiency\n",
        "    ref_embs = model.encode(references)\n",
        "    hyp_embs = model.encode(hypotheses)\n",
        "\n",
        "    # Compute cosine similarity scores for each corresponding pair\n",
        "    similarities = []\n",
        "    for ref_emb, hyp_emb in zip(ref_embs, hyp_embs):\n",
        "        sim = cosine_similarity(ref_emb.reshape(1, -1), hyp_emb.reshape(1, -1))[0][0]\n",
        "        similarities.append(sim)\n",
        "\n",
        "    # Convert list to numpy array for easier statistics computation\n",
        "    similarities = np.array(similarities)\n",
        "    avg_sim = np.mean(similarities)\n",
        "\n",
        "    # Get indices of the min and max similarity values\n",
        "    min_idx = int(np.argmin(similarities))\n",
        "    max_idx = int(np.argmax(similarities))\n",
        "    min_sim = similarities[min_idx]\n",
        "    max_sim = similarities[max_idx]\n",
        "\n",
        "    return {\n",
        "        \"average\": avg_sim,\n",
        "        \"min\": min_sim,\n",
        "        \"min_index\": min_idx,\n",
        "        \"max\": max_sim,\n",
        "        \"max_index\": max_idx\n",
        "    }\n"
      ],
      "metadata": {
        "id": "RSTl7CzizaGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_levenshtein_distance(references, hypotheses):\n",
        "    \"\"\"\n",
        "    Computes Levenshtein distance between corresponding pairs of reference and hypothesis sentences.\n",
        "    Returns the average, minimum, and maximum Levenshtein distance, along with the indices of the pairs\n",
        "    that produced the min and max distances.\n",
        "    \"\"\"\n",
        "    lev_distances = []\n",
        "    for reference, hypothesis in zip(references, hypotheses):\n",
        "        lev_dist = levenshtein_distance(reference, hypothesis)\n",
        "        lev_distances.append(lev_dist)\n",
        "\n",
        "    average_distance = sum(lev_distances) / len(lev_distances)\n",
        "    min_distance = min(lev_distances)\n",
        "    max_distance = max(lev_distances)\n",
        "    min_index = lev_distances.index(min_distance)\n",
        "    max_index = lev_distances.index(max_distance)\n",
        "    return {\n",
        "        \"average\": average_distance,\n",
        "        \"min\": min_distance,\n",
        "        \"min_index\": min_index,\n",
        "        \"max\": max_distance,\n",
        "        \"max_index\": max_index\n",
        "    }"
      ],
      "metadata": {
        "id": "HDssKidK2dob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_llm_based_precision(llm_eval_results):\n",
        "  \"\"\"\n",
        "  Parses LLM evaluation and computes a precision based on this.\n",
        "  \"\"\"\n",
        "  labels = []\n",
        "  for result in llm_eval_results:\n",
        "    if result.startswith((\"так\", \"Так\")):\n",
        "      labels.append(\"yes\")\n",
        "    elif result.startswith((\"ні\", \"Ні\")):\n",
        "      labels.append(\"no\")\n",
        "    else:\n",
        "      labels.append(\"none\")\n",
        "\n",
        "  count_labels = Counter(labels)\n",
        "\n",
        "  precision = count_labels[\"yes\"] / (count_labels[\"yes\"] + count_labels[\"no\"])\n",
        "  return precision"
      ],
      "metadata": {
        "id": "9VPsimCJF5YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt iteration**"
      ],
      "metadata": {
        "id": "Q9hPuFts2B3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks:\n",
        "1. Improve `correction_prompt`. Compare metrics. When Errant improves, how do other metrics behave?\n",
        "  * Try specifying type of the errors we want to be fixed: spelling, grammar, punctuation, etc.\n",
        "  * Try few-shot approach, i.e. providing examples.\n",
        "  * Your ideas (you can try approaches/techniques from the presentation).\n",
        "2. Improve `eval_prompt`. E.g., provide input and output and ask if the correction was done well. Note: you'll have to modify `run_batch` for this task.\n",
        "3. ***Ask the model to provide a reasoning/explanation for the correction. Use json output. Remember to parse json and extract only the corrected sentence when running the metrics.\n",
        "\n",
        "**Instructions**!\n",
        "* Make a copy of this notebook to your Drive and work on it from there.\n",
        "* Use `dev_set` for prompt iteration.\n",
        "* Use `eval_set` only once at the end of the workshop to evaluate your final solution.\n",
        "* At the end of the workshop, we’ll discuss the results—what worked for you and what didn’t. Feel free to share your experience with others 🙏. You can keep an eye on your results in the `workshop_run.log` file.\n"
      ],
      "metadata": {
        "id": "bEHk8h93rUh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correction_prompt = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=\"\"\"\n",
        "    Дано таке речення:\n",
        "    {data}\n",
        "\n",
        "    Завдання - проаналізувати його і виправити граматичні помилки. Поверни лише виправлене речення.\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "3Mfxo8mB2Upr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = PromptTemplate(\n",
        "    input_variables=[\"data\"],\n",
        "    template=\"\"\"\n",
        "    Дано таке речення:\n",
        "    {data}\n",
        "\n",
        "    Оціни чи граматичне воно. Поверни \"так\" або \"ні\".\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "BokpARF8_TvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dev set"
      ],
      "metadata": {
        "id": "tOmvwE_XAjhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "dev_source_data = get_data(\"ua-gec-workshop/data/dev/dev_src_data.txt\")\n",
        "dev_target_data = get_data(\"ua-gec-workshop/data/dev/dev_tgt_data.txt\")"
      ],
      "metadata": {
        "id": "0Sypt1kMuwn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a prompt to fix GEC\n",
        "logger.info(\"Prompt: %s\", correction_prompt.template)\n",
        "dev_corrected_data = run_batch(\"dev_corrected_data.txt\", dev_source_data, correction_prompt)"
      ],
      "metadata": {
        "id": "9cT95sVJ2Ifm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Errant metric\n",
        "errant_stats = compute_errant_metric(\"dev_corrected_data.txt\", \"ua-gec-workshop/data/dev/dev.m2\")\n",
        "logger.info(\"Errant results:\\n%s\", errant_stats)"
      ],
      "metadata": {
        "id": "yic1emAvAHip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Levenstein distance metric\n",
        "lev_stats = compute_levenshtein_distance(dev_target_data, dev_corrected_data)\n",
        "logger.info(\n",
        "    f\"\"\"Levenshtein Distance - Avg: {lev_stats['average']:.2f},\n",
        "     Min: {lev_stats['min']}, Min sentence index: {lev_stats[\"min_index\"]},\n",
        "     Max: {lev_stats['max']}, Max sentence index: {lev_stats[\"max_index\"]}\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "SlV1wknLAggb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Cosimilarity metric\n",
        "sim_stats = compute_cosine_similarity(dev_target_data, dev_corrected_data, transformers_model)\n",
        "logger.info(\n",
        "    f\"\"\"Cosine Similarity - Avg: {sim_stats['average']:.4f},\n",
        "     Min: {sim_stats['min']:.4f}, Min sentence index: {sim_stats[\"min_index\"]},\n",
        "     Max: {sim_stats['max']:.4f}, Max sentence index: {sim_stats[\"max_index\"]}\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "mQ68iyrCu9Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run LLM-based evaluation\n",
        "dev_llm_eval_results = run_batch(\"dev_llm_eval_results.txt\", dev_corrected_data, eval_prompt)"
      ],
      "metadata": {
        "id": "AY985D7BDLIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute LLM-based evaluation\n",
        "llm_eval = compute_llm_based_precision(dev_llm_eval_results)\n",
        "logger.info(f\"LLM eval - {llm_eval}\")"
      ],
      "metadata": {
        "id": "xEC-dK41Gc27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_one_example(\n",
        "    id,\n",
        "    dev_source_data=dev_source_data, dev_target_data=dev_target_data,\n",
        "    dev_corrected_data=dev_corrected_data, model=transformers_model\n",
        "  ):\n",
        "  print(f\"Original sentence: {dev_source_data[id]}\")\n",
        "  print(f\"Target sentence: {dev_target_data[id]}\")\n",
        "  print(f\"Corrected sentence: {dev_corrected_data[id]}\")\n",
        "\n",
        "  levenstein = compute_levenshtein_distance([dev_target_data[id]], [dev_corrected_data[id]])[\"average\"]\n",
        "  similarity = compute_cosine_similarity([dev_target_data[id]], [dev_corrected_data[id]], model)[\"average\"]\n",
        "  print(f\"Levenstein distance: {levenstein}\")\n",
        "  print(f\"Cosine similarity: {similarity}\")\n",
        "\n",
        "  llm_eval = generate_response(eval_prompt.format(data=dev_corrected_data[id]))\n",
        "  print(f\"LLM eval: {llm_eval}\")"
      ],
      "metadata": {
        "id": "PsGIju0lX0v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_one_example(5)"
      ],
      "metadata": {
        "id": "b0ZT-10XbnZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval set"
      ],
      "metadata": {
        "id": "YMbXyPk6Amvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "eval_source_data = get_data(\"ua-gec-workshop/data/eval/eval_src_data.txt\")\n",
        "eval_target_data = get_data(\"ua-gec-workshop/data/eval/eval_tgt_data.txt\")"
      ],
      "metadata": {
        "id": "dTBRvkEDuyFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run a prompt to fix GEC\n",
        "logger.info(\"[Eval data] Prompt: %s\", correction_prompt.template)\n",
        "eval_corrected_data = run_batch(\"eval_corrected_data.txt\", eval_source_data, correction_prompt)"
      ],
      "metadata": {
        "id": "pv3vUfeo2JOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Errant metric\n",
        "errant_stats = compute_errant_metric(\"eval_corrected_data.txt\", \"ua-gec-workshop/data/eval/eval.m2\")\n",
        "logger.info(\"Errant results:\\n%s\", errant_stats)"
      ],
      "metadata": {
        "id": "LEkk8xfEA0aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Levenstein distance metric\n",
        "lev_stats = compute_levenshtein_distance(eval_target_data, eval_corrected_data)\n",
        "logger.info(\n",
        "    f\"\"\"Levenshtein Distance - Avg: {lev_stats['average']:.2f},\n",
        "     Min: {lev_stats['min']}, Min sentence index: {lev_stats[\"min_index\"]},\n",
        "     Max: {lev_stats['max']}, Max sentence index: {lev_stats[\"max_index\"]}\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "kAAAONRZA1bB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Cosimilarity metric\n",
        "sim_stats = compute_cosine_similarity(eval_target_data, eval_corrected_data, transformers_model)\n",
        "logger.info(\n",
        "    f\"\"\"Cosine Similarity - Avg: {sim_stats['average']:.4f},\n",
        "     Min: {sim_stats['min']:.4f}, Min sentence index: {sim_stats[\"min_index\"]},\n",
        "     Max: {sim_stats['max']:.4f}, Max sentence index: {sim_stats[\"max_index\"]}\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "UsLnf_PBvQ3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run LLM-based evaluation\n",
        "eval_llm_eval_results = run_batch(\"eval_llm_eval_results.txt\", eval_corrected_data, eval_prompt)"
      ],
      "metadata": {
        "id": "6pZTWcOaGfXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute LLM-based evaluation\n",
        "llm_eval = compute_llm_based_precision(eval_llm_eval_results)\n",
        "logger.info(f\"LLM eval - {llm_eval}\")"
      ],
      "metadata": {
        "id": "8zS_JE3bGhrd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}